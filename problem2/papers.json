[
  {
    "arxiv_id": "2307.01189v2",
    "title": "Trainable Transformer in Transformer",
    "authors": [
      "Abhishek Panigrahi",
      "Sadhika Malladi",
      "Mengzhou Xia",
      "Sanjeev Arora"
    ],
    "abstract": "Recent works attribute the capability of in-context learning (ICL) in large\npre-trained language models to implicitly simulating and fine-tuning an\ninternal model (e.g., linear or 2-layer MLP) during inference. However, such\nconstructions require large memory overhead, which makes simulation of more\nsophisticated internal models intractable. In this work, we propose an\nefficient construction, Transformer in Transformer (in short, TinT), that\nallows a transformer to simulate and fine-tune complex models internally during\ninference (e.g., pre-trained language models). In particular, we introduce\ninnovative approximation techniques that allow a TinT model with less than 2\nbillion parameters to simulate and fine-tune a 125 million parameter\ntransformer model within a single forward pass. TinT accommodates many common\ntransformer variants and its design ideas also improve the efficiency of past\ninstantiations of simple models inside transformers. We conduct end-to-end\nexperiments to validate the internal fine-tuning procedure of TinT on various\nlanguage modeling and downstream tasks. For example, even with a limited\none-step budget, we observe TinT for a OPT-125M model improves performance by\n4-16% absolute on average compared to OPT-125M. These findings suggest that\nlarge pre-trained language models are capable of performing intricate\nsubroutines. To facilitate further work, a modular and extensible codebase for\nTinT is included.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-07-03T17:53:39Z",
    "updated": "2024-02-08T16:19:14Z",
    "abstract_stats": {
      "total_words": 219,
      "unique_words": 113,
      "total_sentences": 13,
      "avg_words_per_sentence": 16.846153846153847,
      "avg_word_length": 6.651612903225806,
      "top_words": [
        {
          "word": "models",
          "frequency": 6
        },
        {
          "word": "tint",
          "frequency": 6
        },
        {
          "word": "transformer",
          "frequency": 5
        },
        {
          "word": "fine",
          "frequency": 4
        },
        {
          "word": "language",
          "frequency": 4
        },
        {
          "word": "model",
          "frequency": 4
        },
        {
          "word": "internal",
          "frequency": 3
        },
        {
          "word": "large",
          "frequency": 3
        },
        {
          "word": "pre",
          "frequency": 3
        },
        {
          "word": "trained",
          "frequency": 3
        },
        {
          "word": "125m",
          "frequency": 2
        },
        {
          "word": "2",
          "frequency": 2
        },
        {
          "word": "e",
          "frequency": 2
        },
        {
          "word": "end",
          "frequency": 2
        },
        {
          "word": "g",
          "frequency": 2
        },
        {
          "word": "inference",
          "frequency": 2
        },
        {
          "word": "opt",
          "frequency": 2
        },
        {
          "word": "simulate",
          "frequency": 2
        },
        {
          "word": "tune",
          "frequency": 2
        },
        {
          "word": "tuning",
          "frequency": 2
        }
      ],
      "longest_sentence": "In particular, we introduce\ninnovative approximation techniques that allow a TinT model with less than 2\nbillion parameters to simulate and fine-tune a 125 million parameter\ntransformer model within a single forward pass",
      "shortest_sentence": "g"
    }
  },
  {
    "arxiv_id": "2301.13573v1",
    "title": "Skill Decision Transformer",
    "authors": [
      "Shyam Sudhakaran",
      "Sebastian Risi"
    ],
    "abstract": "Recent work has shown that Large Language Models (LLMs) can be incredibly\neffective for offline reinforcement learning (RL) by representing the\ntraditional RL problem as a sequence modelling problem (Chen et al., 2021;\nJanner et al., 2021). However many of these methods only optimize for high\nreturns, and may not extract much information from a diverse dataset of\ntrajectories. Generalized Decision Transformers (GDTs) (Furuta et al., 2021)\nhave shown that utilizing future trajectory information, in the form of\ninformation statistics, can help extract more information from offline\ntrajectory data. Building upon this, we propose Skill Decision Transformer\n(Skill DT). Skill DT draws inspiration from hindsight relabelling (Andrychowicz\net al., 2017) and skill discovery methods to discover a diverse set of\nprimitive behaviors, or skills. We show that Skill DT can not only perform\noffline state-marginal matching (SMM), but can discovery descriptive behaviors\nthat can be easily sampled. Furthermore, we show that through purely\nreward-free optimization, Skill DT is still competitive with supervised offline\nRL approaches on the D4RL benchmark. The code and videos can be found on our\nproject page: https://github.com/shyamsn97/skill-dt",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-01-31T11:52:46Z",
    "updated": "2023-01-31T11:52:46Z",
    "abstract_stats": {
      "total_words": 188,
      "unique_words": 95,
      "total_sentences": 13,
      "avg_words_per_sentence": 14.461538461538462,
      "avg_word_length": 6.458015267175573,
      "top_words": [
        {
          "word": "skill",
          "frequency": 7
        },
        {
          "word": "dt",
          "frequency": 5
        },
        {
          "word": "al",
          "frequency": 4
        },
        {
          "word": "et",
          "frequency": 4
        },
        {
          "word": "information",
          "frequency": 4
        },
        {
          "word": "offline",
          "frequency": 4
        },
        {
          "word": "2021",
          "frequency": 3
        },
        {
          "word": "rl",
          "frequency": 3
        },
        {
          "word": "behaviors",
          "frequency": 2
        },
        {
          "word": "decision",
          "frequency": 2
        },
        {
          "word": "discovery",
          "frequency": 2
        },
        {
          "word": "diverse",
          "frequency": 2
        },
        {
          "word": "extract",
          "frequency": 2
        },
        {
          "word": "methods",
          "frequency": 2
        },
        {
          "word": "problem",
          "frequency": 2
        },
        {
          "word": "show",
          "frequency": 2
        },
        {
          "word": "shown",
          "frequency": 2
        },
        {
          "word": "trajectory",
          "frequency": 2
        },
        {
          "word": "2017",
          "frequency": 1
        },
        {
          "word": "andrychowicz",
          "frequency": 1
        }
      ],
      "longest_sentence": "Recent work has shown that Large Language Models (LLMs) can be incredibly\neffective for offline reinforcement learning (RL) by representing the\ntraditional RL problem as a sequence modelling problem (Chen et al",
      "shortest_sentence": ", 2021)"
    }
  },
  {
    "arxiv_id": "2302.00049v3",
    "title": "Transformers Meet Directed Graphs",
    "authors": [
      "Simon Geisler",
      "Yujia Li",
      "Daniel Mankowitz",
      "Ali Taylan Cemgil",
      "Stephan GÃ¼nnemann",
      "Cosmin Paduraru"
    ],
    "abstract": "Transformers were originally proposed as a sequence-to-sequence model for\ntext but have become vital for a wide range of modalities, including images,\naudio, video, and undirected graphs. However, transformers for directed graphs\nare a surprisingly underexplored topic, despite their applicability to\nubiquitous domains, including source code and logic circuits. In this work, we\npropose two direction- and structure-aware positional encodings for directed\ngraphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware\ngeneralization of the combinatorial Laplacian; (2) directional random walk\nencodings. Empirically, we show that the extra directionality information is\nuseful in various downstream tasks, including correctness testing of sorting\nnetworks and source code understanding. Together with a data-flow-centric graph\nconstruction, our model outperforms the prior state of the art on the Open\nGraph Benchmark Code2 relatively by 14.7%.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-01-31T19:33:14Z",
    "updated": "2023-08-31T14:38:57Z",
    "abstract_stats": {
      "total_words": 137,
      "unique_words": 80,
      "total_sentences": 6,
      "avg_words_per_sentence": 22.833333333333332,
      "avg_word_length": 7.2631578947368425,
      "top_words": [
        {
          "word": "graphs",
          "frequency": 3
        },
        {
          "word": "including",
          "frequency": 3
        },
        {
          "word": "aware",
          "frequency": 2
        },
        {
          "word": "code",
          "frequency": 2
        },
        {
          "word": "directed",
          "frequency": 2
        },
        {
          "word": "direction",
          "frequency": 2
        },
        {
          "word": "encodings",
          "frequency": 2
        },
        {
          "word": "graph",
          "frequency": 2
        },
        {
          "word": "laplacian",
          "frequency": 2
        },
        {
          "word": "model",
          "frequency": 2
        },
        {
          "word": "sequence",
          "frequency": 2
        },
        {
          "word": "source",
          "frequency": 2
        },
        {
          "word": "transformers",
          "frequency": 2
        },
        {
          "word": "1",
          "frequency": 1
        },
        {
          "word": "14",
          "frequency": 1
        },
        {
          "word": "2",
          "frequency": 1
        },
        {
          "word": "7",
          "frequency": 1
        },
        {
          "word": "applicability",
          "frequency": 1
        },
        {
          "word": "art",
          "frequency": 1
        },
        {
          "word": "audio",
          "frequency": 1
        }
      ],
      "longest_sentence": "In this work, we\npropose two direction- and structure-aware positional encodings for directed\ngraphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware\ngeneralization of the combinatorial Laplacian; (2) directional random walk\nencodings",
      "shortest_sentence": "7%"
    }
  },
  {
    "arxiv_id": "2311.01906v2",
    "title": "Simplifying Transformer Blocks",
    "authors": [
      "Bobby He",
      "Thomas Hofmann"
    ],
    "abstract": "A simple design recipe for deep Transformers is to compose identical building\nblocks. But standard transformer blocks are far from simple, interweaving\nattention and MLP sub-blocks with skip connections & normalisation layers in\nprecise arrangements. This complexity leads to brittle architectures, where\nseemingly minor changes can significantly reduce training speed, or render\nmodels untrainable.\n  In this work, we ask to what extent the standard transformer block can be\nsimplified? Combining signal propagation theory and empirical observations, we\nmotivate modifications that allow many block components to be removed with no\nloss of training speed, including skip connections, projection or value\nparameters, sequential sub-blocks and normalisation layers. In experiments on\nboth autoregressive decoder-only and BERT encoder-only models, our simplified\ntransformers emulate the per-update training speed and performance of standard\ntransformers, while enjoying 15% faster training throughput, and using 15%\nfewer parameters.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-11-03T13:30:52Z",
    "updated": "2024-05-31T11:14:16Z",
    "abstract_stats": {
      "total_words": 143,
      "unique_words": 77,
      "total_sentences": 6,
      "avg_words_per_sentence": 23.833333333333332,
      "avg_word_length": 7.405940594059406,
      "top_words": [
        {
          "word": "blocks",
          "frequency": 4
        },
        {
          "word": "training",
          "frequency": 4
        },
        {
          "word": "speed",
          "frequency": 3
        },
        {
          "word": "standard",
          "frequency": 3
        },
        {
          "word": "transformers",
          "frequency": 3
        },
        {
          "word": "15",
          "frequency": 2
        },
        {
          "word": "block",
          "frequency": 2
        },
        {
          "word": "connections",
          "frequency": 2
        },
        {
          "word": "layers",
          "frequency": 2
        },
        {
          "word": "models",
          "frequency": 2
        },
        {
          "word": "normalisation",
          "frequency": 2
        },
        {
          "word": "parameters",
          "frequency": 2
        },
        {
          "word": "simple",
          "frequency": 2
        },
        {
          "word": "simplified",
          "frequency": 2
        },
        {
          "word": "skip",
          "frequency": 2
        },
        {
          "word": "sub",
          "frequency": 2
        },
        {
          "word": "transformer",
          "frequency": 2
        },
        {
          "word": "allow",
          "frequency": 1
        },
        {
          "word": "architectures",
          "frequency": 1
        },
        {
          "word": "arrangements",
          "frequency": 1
        }
      ],
      "longest_sentence": "Combining signal propagation theory and empirical observations, we\nmotivate modifications that allow many block components to be removed with no\nloss of training speed, including skip connections, projection or value\nparameters, sequential sub-blocks and normalisation layers",
      "shortest_sentence": "A simple design recipe for deep Transformers is to compose identical building\nblocks"
    }
  },
  {
    "arxiv_id": "2302.14346v1",
    "title": "Sampled Transformer for Point Sets",
    "authors": [
      "Shidi Li",
      "Christian Walder",
      "Alexander Soen",
      "Lexing Xie",
      "Miaomiao Liu"
    ],
    "abstract": "The sparse transformer can reduce the computational complexity of the\nself-attention layers to $O(n)$, whilst still being a universal approximator of\ncontinuous sequence-to-sequence functions. However, this permutation variant\noperation is not appropriate for direct application to sets. In this paper, we\nproposed an $O(n)$ complexity sampled transformer that can process point set\nelements directly without any additional inductive bias. Our sampled\ntransformer introduces random element sampling, which randomly splits point\nsets into subsets, followed by applying a shared Hamiltonian self-attention\nmechanism to each subset. The overall attention mechanism can be viewed as a\nHamiltonian cycle in the complete attention graph, and the permutation of point\nset elements is equivalent to randomly sampling Hamiltonian cycles. This\nmechanism implements a Monte Carlo simulation of the $O(n^2)$ dense attention\nconnections. We show that it is a universal approximator for continuous\nset-to-set functions. Experimental results on point-clouds show comparable or\nbetter accuracy with significantly reduced computational complexity compared to\nthe dense transformer or alternative sparse attention schemes.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-02-28T06:38:05Z",
    "updated": "2023-02-28T06:38:05Z",
    "abstract_stats": {
      "total_words": 174,
      "unique_words": 78,
      "total_sentences": 8,
      "avg_words_per_sentence": 21.75,
      "avg_word_length": 7.322033898305085,
      "top_words": [
        {
          "word": "attention",
          "frequency": 6
        },
        {
          "word": "point",
          "frequency": 4
        },
        {
          "word": "set",
          "frequency": 4
        },
        {
          "word": "transformer",
          "frequency": 4
        },
        {
          "word": "complexity",
          "frequency": 3
        },
        {
          "word": "hamiltonian",
          "frequency": 3
        },
        {
          "word": "mechanism",
          "frequency": 3
        },
        {
          "word": "n",
          "frequency": 3
        },
        {
          "word": "o",
          "frequency": 3
        },
        {
          "word": "approximator",
          "frequency": 2
        },
        {
          "word": "computational",
          "frequency": 2
        },
        {
          "word": "continuous",
          "frequency": 2
        },
        {
          "word": "dense",
          "frequency": 2
        },
        {
          "word": "elements",
          "frequency": 2
        },
        {
          "word": "functions",
          "frequency": 2
        },
        {
          "word": "permutation",
          "frequency": 2
        },
        {
          "word": "randomly",
          "frequency": 2
        },
        {
          "word": "sampled",
          "frequency": 2
        },
        {
          "word": "sampling",
          "frequency": 2
        },
        {
          "word": "self",
          "frequency": 2
        }
      ],
      "longest_sentence": "The overall attention mechanism can be viewed as a\nHamiltonian cycle in the complete attention graph, and the permutation of point\nset elements is equivalent to randomly sampling Hamiltonian cycles",
      "shortest_sentence": "However, this permutation variant\noperation is not appropriate for direct application to sets"
    }
  }
]